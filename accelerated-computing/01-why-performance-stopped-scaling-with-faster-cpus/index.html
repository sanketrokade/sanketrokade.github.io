<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="title" content="Why Performance Stopped Scaling with Faster CPUs">
  <meta name="series" content="Accelerated Computing — From First Principles">
  <meta name="order" content="1">
  <title>Why Performance Stopped Scaling with Faster CPUs</title>
  <link rel="stylesheet" href="/assets/style.css">
</head>
<body>

  <h1>Why Performance Stopped Scaling with Faster CPUs</h1>

  <p><em>Accelerated Computing — From First Principles (Part 1)</em></p>
  <hr>
  <p>
    For a long time, using a computer felt almost magical.
  </p>

  <p>
    You bought a new machine, installed the same software, and everything just ran faster.
    No code changes. No optimizations. No effort.
  </p>

  <p>
    This pattern held for decades — so consistently that many people assumed it would last forever.
  </p>

  <p>
    But eventually, the free performance stopped coming.
  </p>

  <p>
    Modern CPUs are astonishing feats of engineering, containing billions of transistors and layers of sophisticated logic.
    Yet the speed of everyday programs no longer improves the way it once did.
  </p>

  <p>
    This didn’t happen because engineers ran out of ideas.
    It happened because we ran into fundamental limits.
  </p>

  <p>
    To understand accelerated computing, GPUs, and modern hardware design, we must first understand why faster CPUs stopped being the answer.
  </p>

  <hr>

  <h2>1. The Era of “Free” Performance</h2>

  <p>
    From roughly the early 1990s to the mid-2000s, performance gains came almost automatically.
  </p>

  <p>Every new processor generation delivered:</p>

  <ul>
    <li>Higher clock speeds</li>
    <li>Deeper and smarter execution pipelines</li>
    <li>Increasingly clever hardware optimizations</li>
  </ul>

  <p>
    Software developers benefited without doing anything special.
    If your program took 10 seconds this year, it might take 7 seconds next year — simply because the CPU was faster.
  </p>

  <p>This led to an implicit assumption:</p>

  <blockquote>
    <strong>Performance improvements are the hardware’s responsibility.</strong>
  </blockquote>

  <p>
    That assumption held for a long time — until it broke.
  </p>

  <p>
    What made this era possible was <strong>Dennard scaling</strong>: as transistors shrank, they became faster
    <em>and</em> more power-efficient.
    Combined with <strong>Moore’s Law</strong>, which kept transistor counts doubling,
    CPUs could increase clock speeds without exploding power consumption.
  </p>

  <p>
    But neither law would hold forever.
  </p>

  <hr>

  <h2>2. What Does “Faster CPU” Actually Mean?</h2>

  <p>
    At a high level, a CPU executes instructions in a sequence:
  </p>

  <ol>
    <li>Load data</li>
    <li>Perform an operation</li>
    <li>Store the result</li>
  </ol>

  <p>
    One of the most direct ways to make this faster is to increase the clock frequency —
    measured in megahertz (MHz) and later gigahertz (GHz).
  </p>

  <p>A higher clock frequency means:</p>

  <ul>
    <li>More instruction steps per second</li>
    <li>Less time per individual operation</li>
  </ul>

  <p>
    For many years, performance improvements were largely driven by steadily increasing clock speeds.
  </p>

  <p>
    So why didn’t we just keep increasing them?
  </p>

  <hr>

  <h2>3. The Clock Frequency Wall</h2>

  <p>
    Increasing clock frequency is not free.
  </p>

  <p>As frequency rises:</p>

  <ul>
    <li>Power consumption increases sharply</li>
    <li>Heat generation rises with it</li>
    <li>Signal timing becomes harder to control</li>
  </ul>

  <p>
    Eventually, CPUs began to consume too much power and generate too much heat to remain practical.
    Cooling solutions became complex, expensive, and inefficient.
  </p>

  <p>
    This barrier is often called the <strong>power wall</strong>.
  </p>

  <p>
    Engineers did not forget how to raise frequencies — they simply could not do so without creating chips that:
  </p>

  <ul>
    <li>Overheated quickly</li>
    <li>Drew enormous amounts of power</li>
    <li>Became unreliable at scale</li>
  </ul>

  <p>
    Physics imposed a hard constraint.
  </p>

  <p>
    Clock speeds stopped scaling exponentially, even though transistor counts continued to rise.
  </p>

  <hr>

  <h2>4. Getting More Work from Each Clock Cycle</h2>

  <p>
    When frequency scaling slowed, CPU designers turned to another strategy:
    <strong>do more work per clock cycle</strong>.
  </p>

  <p>This led to techniques such as:</p>

  <ul>
    <li>Executing multiple instructions at once</li>
    <li>Reordering instructions internally</li>
    <li>Predicting which branches and data will be needed next</li>
  </ul>

  <p>
    Together, these techniques are known as <strong>Instruction-Level Parallelism (ILP)</strong>.
  </p>

  <p>
    From the programmer’s perspective, the code still looked sequential.
    Internally, however, the CPU was aggressively rearranging and speculating
    to keep its execution units busy.
  </p>

  <p>
    For a while, this worked.
  </p>

  <hr>

  <h2>5. The Limits of Instruction-Level Parallelism</h2>

  <p>
    Real programs are not infinitely parallel.
  </p>

  <p>They contain:</p>

  <ul>
    <li>Data dependencies</li>
    <li>Branches and control flow</li>
    <li>Memory accesses with unpredictable latency</li>
  </ul>

  <p>
    As CPUs grew more complex, extracting ever-smaller amounts of parallelism required
    more transistors, more power, and more design complexity.
  </p>

  <p>
    Each additional percent of performance came at a disproportionately higher cost.
    Linear gains turned into exponential expense.
  </p>

  <p>
    Eventually, the return on investment collapsed.
  </p>

  <hr>

  <h2>6. The Memory Wall</h2>

  <p>
    Even when a CPU has instructions ready to execute, it often spends its time waiting —
    not on computation, but on <strong>data</strong>.
  </p>

  <p>
    Processor speeds increased far faster than memory latency and bandwidth.
    Fetching data from main memory can take hundreds of clock cycles,
    leaving execution units idle despite gigahertz clock speeds.
  </p>

  <p>
    This gap is known as the <strong>memory wall</strong>.
  </p>

  <p>
    Much of modern CPU complexity exists not to perform more arithmetic,
    but to <em>hide memory latency</em>.
    When that hiding fails, performance stalls.
  </p>

  <hr>

  <h2>7. The Multi-Core Shift — and Its Reality</h2>

  <p>
    When single-core performance stopped scaling, the industry changed direction.
  </p>

  <p>Instead of building one extremely fast core, CPUs began shipping with:</p>

  <ul>
    <li>2, 4, 8, or more cores</li>
  </ul>

  <p>
    In theory, this should have solved the problem.
  </p>

  <p>In practice, it exposed another hard truth:</p>

  <blockquote>
    <strong>Not all programs can be parallelized.</strong>
  </blockquote>

  <p>
    Some parts of a program must run sequentially.
    No matter how many cores you add, those sections do not get faster.
  </p>

  <p>
    This idea is captured by <strong>Amdahl’s Law</strong>.
  </p>

  <hr>

  <h2>8. Dark Silicon and Wasted Potential</h2>

  <p>
    Modern CPUs contain billions of transistors, but:
  </p>

  <ul>
    <li>Not all parts of the chip can be active at once</li>
    <li>Power and thermal limits force large regions to remain idle</li>
  </ul>

  <p>
    This phenomenon is often called <strong>dark silicon</strong>.
  </p>

  <p>
    The result is unavoidable inefficiency.
  </p>

  <hr>

  <h2>9. The Uncomfortable Truth About CPUs</h2>

  <p>
    General-purpose CPUs are engineering masterpieces,
    but that flexibility comes at a cost.
  </p>

  <p>For many modern workloads, CPUs:</p>

  <ul>
    <li>Spend enormous energy managing complexity</li>
    <li>Waste power on generality</li>
    <li>Struggle to scale performance efficiently</li>
  </ul>

  <hr>

  <h2>10. The Only Viable Path Forward</h2>

  <p>
    Once frequency scaling stalled and general-purpose optimization hit hard limits,
    the choice became unavoidable.
  </p>

  <ul>
    <li>Accept diminishing returns</li>
    <li>Or trade generality for efficiency</li>
  </ul>

  <p>
    This realization made specialized accelerators inevitable.
  </p>

  <hr>

  <h2>What Comes Next</h2>

  <p>
    If CPUs cannot efficiently scale performance alone, the next question is obvious:
  </p>

  <p>
    Where does the real cost of computation lie?
  </p>

  <p>
    That is where we go next.
  </p>

  <h3>Next Article</h3>
  <p>
    <strong>The Real Cost of Computation: Data Movement vs Arithmetic</strong>
  </p>

</body>
</html>
