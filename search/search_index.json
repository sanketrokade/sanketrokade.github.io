{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/","text":"Why Performance Stopped Scaling with Faster CPUs For a long time, using a computer felt almost magical. You bought a new machine, installed the same software, and everything just ran faster. No code changes. No optimizations. No effort. This pattern held for decades \u2014 so consistently that many people assumed it would last forever. But eventually, the free performance stopped coming. Modern CPUs are astonishing feats of engineering, containing billions of transistors and layers of sophisticated logic. Yet the speed of everyday programs no longer improves the way it once did. This didn\u2019t happen because engineers ran out of ideas. It happened because we ran into fundamental limits. To understand accelerated computing, GPUs, and modern hardware design, we must first understand why faster CPUs stopped being the answer. 1. The Era of \u201cFree\u201d Performance From roughly the early 1990s to the mid-2000s, performance gains came almost automatically. Every new processor generation delivered: Higher clock speeds Deeper and smarter execution pipelines Increasingly clever hardware optimizations Software developers benefited without doing anything special. If your program took 10 seconds this year, it might take 7 seconds next year \u2014 simply because the CPU was faster. This led to an implicit assumption: Performance improvements are the hardware\u2019s responsibility. That assumption held for a long time \u2014 until it broke. What made this era possible was Dennard scaling : as transistors shrank, they became faster and more power-efficient. Combined with Moore\u2019s Law , which kept transistor counts doubling, CPUs could increase clock speeds without exploding power consumption. But neither law would hold forever. 2. What Does \u201cFaster CPU\u201d Actually Mean? At a high level, a CPU executes instructions in a sequence: Load data Perform an operation Store the result One of the most direct ways to make this faster is to increase the clock frequency \u2014 measured in megahertz (MHz) and later gigahertz (GHz). A higher clock frequency means: More instruction steps per second Less time per individual operation For many years, performance improvements were largely driven by steadily increasing clock speeds. So why didn\u2019t we just keep increasing them? 3. The Clock Frequency Wall Increasing clock frequency is not free. As frequency rises: Power consumption increases sharply Heat generation rises with it Signal timing becomes harder to control Eventually, CPUs began to consume too much power and generate too much heat to remain practical. Cooling solutions became complex, expensive, and inefficient. This barrier is often called the power wall . Engineers did not forget how to raise frequencies \u2014 they simply could not do so without creating chips that: Overheated quickly Drew enormous amounts of power Became unreliable at scale Physics imposed a hard constraint. Clock speeds stopped scaling exponentially, even though transistor counts continued to rise. 4. Getting More Work from Each Clock Cycle When frequency scaling slowed, CPU designers turned to another strategy: do more work per clock cycle . This led to techniques such as: Executing multiple instructions at once Reordering instructions internally Predicting which branches and data will be needed next Together, these techniques are known as Instruction-Level Parallelism (ILP) . From the programmer\u2019s perspective, the code still looked sequential. Internally, however, the CPU was aggressively rearranging and speculating to keep its execution units busy. For a while, this worked. 5. The Limits of Instruction-Level Parallelism Real programs are not infinitely parallel. They contain: Data dependencies (one result needed before the next step) Branches and control flow Memory accesses with unpredictable latency As CPUs grew more complex, extracting ever-smaller amounts of parallelism required: More transistors More power More design complexity Each additional percent of performance came at a disproportionately higher cost. Linear gains turned into exponential expense. Eventually, the return on investment collapsed. 6. The Memory Wall Even when a CPU has instructions ready to execute, it often spends its time waiting \u2014 not on computation, but on data . Processor speeds increased far faster than memory latency and bandwidth. Fetching data from main memory can take hundreds of clock cycles, leaving execution units idle despite gigahertz clock speeds. This gap is known as the memory wall . Much of modern CPU complexity \u2014 deep caches, speculation, out-of-order execution \u2014 exists not to perform more arithmetic, but to hide memory latency . When that hiding fails, performance stalls. At this point, adding more cleverness yielded diminishing returns. 7. The Multi-Core Shift \u2014 and Its Reality When single-core performance stopped scaling, the industry changed direction. Instead of building one extremely fast core, CPUs began shipping with: 2, 4, 8, or more cores In theory, this should have solved the problem. In practice, it exposed another hard truth: Not all programs can be parallelized. Some parts of a program must run sequentially. No matter how many cores you add, those sections do not get faster. This idea is captured by Amdahl\u2019s Law , which states that the serial portion of a program limits its maximum speedup. Multi-core processors help \u2014 but only if software is explicitly designed to use them. 8. Dark Silicon and Wasted Potential Modern CPUs contain billions of transistors, but: Not all parts of the chip can be active at once Power and thermal limits force large regions to remain idle This phenomenon is often called dark silicon . It does not mean CPUs are poorly designed. General-purpose processors must support a wide range of features, even if most applications use only a small subset of them. The result is unavoidable inefficiency. 9. The Uncomfortable Truth About CPUs General-purpose CPUs are engineering masterpieces. They are flexible, programmable, and capable of running almost any kind of software. But that flexibility comes at a cost. For many modern workloads, CPUs: Spend enormous energy managing complexity Waste power on generality Struggle to scale performance efficiently CPUs did not stop improving. They continued optimizing for low-latency, single-thread performance \u2014 even as the dominant workloads shifted toward throughput, parallelism, and efficiency. This mismatch made alternatives inevitable. 10. The Only Viable Path Forward Once frequency scaling stalled and general-purpose optimization hit hard limits, the choice became unavoidable: Accept diminishing returns Or trade generality for efficiency The path forward required: Explicit parallelism Specialized hardware Designs optimized for throughput and energy efficiency This realization did not immediately produce GPUs and accelerators \u2014 but it made them inevitable. Understanding this shift is the foundation for everything that follows in accelerated computing. What Comes Next If CPUs cannot efficiently scale performance alone, the next question is obvious: Where does the real cost of computation lie? Is it arithmetic? Or is it something else entirely? That is where we go next. Next Article The Real Cost of Computation: Data Movement vs Arithmetic","title":"Why Performance Stopped Scaling with Faster CPUs"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#why-performance-stopped-scaling-with-faster-cpus","text":"For a long time, using a computer felt almost magical. You bought a new machine, installed the same software, and everything just ran faster. No code changes. No optimizations. No effort. This pattern held for decades \u2014 so consistently that many people assumed it would last forever. But eventually, the free performance stopped coming. Modern CPUs are astonishing feats of engineering, containing billions of transistors and layers of sophisticated logic. Yet the speed of everyday programs no longer improves the way it once did. This didn\u2019t happen because engineers ran out of ideas. It happened because we ran into fundamental limits. To understand accelerated computing, GPUs, and modern hardware design, we must first understand why faster CPUs stopped being the answer.","title":"Why Performance Stopped Scaling with Faster CPUs"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#1-the-era-of-free-performance","text":"From roughly the early 1990s to the mid-2000s, performance gains came almost automatically. Every new processor generation delivered: Higher clock speeds Deeper and smarter execution pipelines Increasingly clever hardware optimizations Software developers benefited without doing anything special. If your program took 10 seconds this year, it might take 7 seconds next year \u2014 simply because the CPU was faster. This led to an implicit assumption: Performance improvements are the hardware\u2019s responsibility. That assumption held for a long time \u2014 until it broke. What made this era possible was Dennard scaling : as transistors shrank, they became faster and more power-efficient. Combined with Moore\u2019s Law , which kept transistor counts doubling, CPUs could increase clock speeds without exploding power consumption. But neither law would hold forever.","title":"1. The Era of \u201cFree\u201d Performance"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#2-what-does-faster-cpu-actually-mean","text":"At a high level, a CPU executes instructions in a sequence: Load data Perform an operation Store the result One of the most direct ways to make this faster is to increase the clock frequency \u2014 measured in megahertz (MHz) and later gigahertz (GHz). A higher clock frequency means: More instruction steps per second Less time per individual operation For many years, performance improvements were largely driven by steadily increasing clock speeds. So why didn\u2019t we just keep increasing them?","title":"2. What Does \u201cFaster CPU\u201d Actually Mean?"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#3-the-clock-frequency-wall","text":"Increasing clock frequency is not free. As frequency rises: Power consumption increases sharply Heat generation rises with it Signal timing becomes harder to control Eventually, CPUs began to consume too much power and generate too much heat to remain practical. Cooling solutions became complex, expensive, and inefficient. This barrier is often called the power wall . Engineers did not forget how to raise frequencies \u2014 they simply could not do so without creating chips that: Overheated quickly Drew enormous amounts of power Became unreliable at scale Physics imposed a hard constraint. Clock speeds stopped scaling exponentially, even though transistor counts continued to rise.","title":"3. The Clock Frequency Wall"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#4-getting-more-work-from-each-clock-cycle","text":"When frequency scaling slowed, CPU designers turned to another strategy: do more work per clock cycle . This led to techniques such as: Executing multiple instructions at once Reordering instructions internally Predicting which branches and data will be needed next Together, these techniques are known as Instruction-Level Parallelism (ILP) . From the programmer\u2019s perspective, the code still looked sequential. Internally, however, the CPU was aggressively rearranging and speculating to keep its execution units busy. For a while, this worked.","title":"4. Getting More Work from Each Clock Cycle"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#5-the-limits-of-instruction-level-parallelism","text":"Real programs are not infinitely parallel. They contain: Data dependencies (one result needed before the next step) Branches and control flow Memory accesses with unpredictable latency As CPUs grew more complex, extracting ever-smaller amounts of parallelism required: More transistors More power More design complexity Each additional percent of performance came at a disproportionately higher cost. Linear gains turned into exponential expense. Eventually, the return on investment collapsed.","title":"5. The Limits of Instruction-Level Parallelism"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#6-the-memory-wall","text":"Even when a CPU has instructions ready to execute, it often spends its time waiting \u2014 not on computation, but on data . Processor speeds increased far faster than memory latency and bandwidth. Fetching data from main memory can take hundreds of clock cycles, leaving execution units idle despite gigahertz clock speeds. This gap is known as the memory wall . Much of modern CPU complexity \u2014 deep caches, speculation, out-of-order execution \u2014 exists not to perform more arithmetic, but to hide memory latency . When that hiding fails, performance stalls. At this point, adding more cleverness yielded diminishing returns.","title":"6. The Memory Wall"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#7-the-multi-core-shift-and-its-reality","text":"When single-core performance stopped scaling, the industry changed direction. Instead of building one extremely fast core, CPUs began shipping with: 2, 4, 8, or more cores In theory, this should have solved the problem. In practice, it exposed another hard truth: Not all programs can be parallelized. Some parts of a program must run sequentially. No matter how many cores you add, those sections do not get faster. This idea is captured by Amdahl\u2019s Law , which states that the serial portion of a program limits its maximum speedup. Multi-core processors help \u2014 but only if software is explicitly designed to use them.","title":"7. The Multi-Core Shift \u2014 and Its Reality"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#8-dark-silicon-and-wasted-potential","text":"Modern CPUs contain billions of transistors, but: Not all parts of the chip can be active at once Power and thermal limits force large regions to remain idle This phenomenon is often called dark silicon . It does not mean CPUs are poorly designed. General-purpose processors must support a wide range of features, even if most applications use only a small subset of them. The result is unavoidable inefficiency.","title":"8. Dark Silicon and Wasted Potential"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#9-the-uncomfortable-truth-about-cpus","text":"General-purpose CPUs are engineering masterpieces. They are flexible, programmable, and capable of running almost any kind of software. But that flexibility comes at a cost. For many modern workloads, CPUs: Spend enormous energy managing complexity Waste power on generality Struggle to scale performance efficiently CPUs did not stop improving. They continued optimizing for low-latency, single-thread performance \u2014 even as the dominant workloads shifted toward throughput, parallelism, and efficiency. This mismatch made alternatives inevitable.","title":"9. The Uncomfortable Truth About CPUs"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#10-the-only-viable-path-forward","text":"Once frequency scaling stalled and general-purpose optimization hit hard limits, the choice became unavoidable: Accept diminishing returns Or trade generality for efficiency The path forward required: Explicit parallelism Specialized hardware Designs optimized for throughput and energy efficiency This realization did not immediately produce GPUs and accelerators \u2014 but it made them inevitable. Understanding this shift is the foundation for everything that follows in accelerated computing.","title":"10. The Only Viable Path Forward"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#what-comes-next","text":"If CPUs cannot efficiently scale performance alone, the next question is obvious: Where does the real cost of computation lie? Is it arithmetic? Or is it something else entirely? That is where we go next.","title":"What Comes Next"},{"location":"accelerated-computing/01-why-performance-stopped-scaling-with-faster-cpus/#next-article","text":"The Real Cost of Computation: Data Movement vs Arithmetic","title":"Next Article"},{"location":"supercomputing/","text":"Supercomputing For Everyone This section indexes my LinkedIn newsletter \u201cSupercomputing For Everyone\u201d . The articles are organized quarter-wise and focus on building conceptual understanding of HPC systems. These articles remain hosted on LinkedIn.","title":"Overview"},{"location":"supercomputing/#supercomputing-for-everyone","text":"This section indexes my LinkedIn newsletter \u201cSupercomputing For Everyone\u201d . The articles are organized quarter-wise and focus on building conceptual understanding of HPC systems. These articles remain hosted on LinkedIn.","title":"Supercomputing For Everyone"}]}